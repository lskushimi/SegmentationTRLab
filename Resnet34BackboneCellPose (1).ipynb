{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82cefac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca54c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss functions from\n",
    "# https://towardsdatascience.com/how-accurate-is-image-segmentation-dd448f896388#:~:text=Dice%20Coefficient&text=Dice%20coefficient%20is%20a%20measure,while%200%20indicates%20no%20overlap.&text=Dice%20Loss%20%3D%201%20%E2%80%94%20Dice%20Coefficient.\n",
    "def dice_metric(inputs, target):\n",
    "    intersection = 2.0 * (target * inputs).sum()\n",
    "    union = target.sum() + inputs.sum()\n",
    "    if target.sum() == 0 and inputs.sum() == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "def dice_loss(inputs, target):\n",
    "    num = target.size(0)\n",
    "    inputs = inputs.reshape(num, -1)\n",
    "    target = target.reshape(num, -1)\n",
    "    smooth = 1.0\n",
    "    intersection = (inputs * target)\n",
    "    dice = (2. * intersection.sum(1) + smooth) / (inputs.sum(1) + target.sum(1) + smooth)\n",
    "    dice = 1 - dice.sum() / num\n",
    "    return dice\n",
    "\n",
    "def bce_dice_loss(inputs, target):\n",
    "    # add sigmoid since we take it out to use BCEWithLogitsLoss\n",
    "    dicescore = dice_loss(nn.functional.sigmoid(inputs), target)\n",
    "    bcescore = nn.BCEWithLogitsLoss()\n",
    "    bceloss = bcescore(inputs, target)\n",
    "\n",
    "    return bceloss + dicescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d284ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway for image folder for training\n",
    "imagePath = '/project/trlab/CellposeDataset/Train'\n",
    "maskPath = '/project/trlab/CellposeDataset/Mask'\n",
    "\n",
    "imagePaths = []\n",
    "maskPaths = []\n",
    "for data_path in sorted(glob.glob(imagePath + '/*')):\n",
    "    imagePaths.append(data_path)\n",
    "    \n",
    "for data_path in sorted(glob.glob(maskPath + '/*')):\n",
    "    maskPaths.append(data_path)\n",
    "    \n",
    "# pathway for image folder for validation\n",
    "imagePath = '/project/trlab/CellposeDataset/Test'\n",
    "maskPath = '/project/trlab/CellposeDataset/TestMask'\n",
    "\n",
    "valImagePaths = []\n",
    "valMaskPaths = []\n",
    "for data_path in sorted(glob.glob(imagePath + '/*')):\n",
    "    valImagePaths.append(data_path)\n",
    "    \n",
    "for data_path in sorted(glob.glob(maskPath + '/*')):\n",
    "    valMaskPaths.append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67e29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, imagePaths, maskPaths):\n",
    "        # init method takes list of image paths, ground truth masks, and transformations as input\n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = maskPaths\n",
    "        \n",
    "    def transform(self, image, mask):\n",
    "        # standardize to values between 0 and 1 for faster convergence\n",
    "        image = image/255.0\n",
    "        image = image.astype('float16')\n",
    "        \n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        # Transfer to device\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        # Random crop (not gonna do this for this run through since the images are all different sizes)\n",
    "        #i, j, h, w = T.RandomCrop.get_params(image, output_size=(512, 512))\n",
    "        #image = TF.crop(image, i, j, h, w)\n",
    "        #mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "            \n",
    "        # Random rotation\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.randint(-90,90)\n",
    "            image = TF.rotate(image,angle)\n",
    "            mask = TF.rotate(mask,angle)\n",
    "            \n",
    "        # Random Gaussian Blur\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.gaussian_blur(image, 3, 0.15)\n",
    "            \n",
    "        # Random sharpness\n",
    "        sharpness = random.random()\n",
    "        shift = random.uniform(0.2,1.8)\n",
    "        if sharpness > 0.5:\n",
    "            image = TF.adjust_sharpness(image, shift)\n",
    "            \n",
    "        # Random contrast not sure why this isn't working, current documentation says one channel tensors are fine\n",
    "        # but it's throwing an error saying you need 3 channels\n",
    "        #contrast = random.random()\n",
    "        #if contrast > 0.5:\n",
    "        #    image = TF.adjust_contrast(image, 2)\n",
    "        #elif contrast < 0.25:\n",
    "        #    image = TF.adjust_contrast(image, 0.5)\n",
    "        \n",
    "        # Random brightness\n",
    "        brightness = random.random()\n",
    "        shift = random.uniform(0.2,1.8)\n",
    "        if brightness > 0.5:\n",
    "            image = TF.adjust_brightness(image, shift)\n",
    "            \n",
    "        # Random Affine\n",
    "        shearx = random.randint(-45,45)\n",
    "        sheary = random.randint(-45,45)\n",
    "        transx = random.randint(-20,20)\n",
    "        transy = random.randint(-20,20)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.affine(image, translate=(transx,transy), shear=(shearx,sheary), angle=0, scale=1)\n",
    "            mask = TF.affine(mask, translate=(transx,transy), shear=(shearx,sheary), angle=0, scale=1)\n",
    "            \n",
    "        # pad to correct size for UNet\n",
    "        #image, pads = pad_to(image,32)\n",
    "        #mask, pads = pad_to(mask,32)\n",
    "        image = TF.center_crop(image, [512,512])\n",
    "        mask = TF.center_crop(mask, [512,512])\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of image paths in dataset\n",
    "        return len(self.imagePaths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # returns sample from dataset\n",
    "        imagePath = self.imagePaths[idx]\n",
    "        maskPath = self.maskPaths[idx]\n",
    "        \n",
    "        image = cv2.imdecode(np.fromfile(imagePath, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)\n",
    "        #mask = cv2.imdecode(np.fromfile(maskPath, dtype=np.uint16), cv2.IMREAD_UNCHANGED)\n",
    "        mask = Image.open(maskPath)\n",
    "        mask = np.array(mask)\n",
    "        mask = cv2.convertScaleAbs(mask) # convert to uint8\n",
    "        ret, mask = cv2.threshold(mask, 0, 1, cv2.THRESH_BINARY) # binarize the mask\n",
    "\n",
    "        # use RandomChoice with only ony composed transform so it applies the same way to both\n",
    "        #transform = T.RandomChoice([T.Compose([T.AutoAugment(T.AutoAugmentPolicy.IMAGENET), T.ToTensor()])])\n",
    "        image, mask = self.transform(image, mask)\n",
    "            \n",
    "        return (image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edec5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "class ValDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, imagePaths, maskPaths):\n",
    "        # init method takes list of image paths, ground truth masks, and transformations as input\n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = maskPaths\n",
    "        \n",
    "    def transform(self, image, mask):\n",
    "        # standardize to values between 0 and 1 for faster convergence\n",
    "        image = image/255.0\n",
    "        image = image.astype('float32')\n",
    "        \n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        # Transfer to device\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        image = TF.center_crop(image, [512,512])\n",
    "        mask = TF.center_crop(mask, [512,512])\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of image paths in dataset\n",
    "        return len(self.imagePaths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # returns sample from dataset\n",
    "        imagePath = self.imagePaths[idx]\n",
    "        maskPath = self.maskPaths[idx]\n",
    "        \n",
    "        image = cv2.imdecode(np.fromfile(imagePath, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = Image.open(maskPath)\n",
    "        mask = np.array(mask)\n",
    "        mask = cv2.convertScaleAbs(mask) # convert to uint8\n",
    "        ret, mask = cv2.threshold(mask, 0, 1, cv2.THRESH_BINARY) # binarize the mask\n",
    "\n",
    "        image, mask = self.transform(image, mask)\n",
    "            \n",
    "        return (image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f53f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "train_dataset = TrainDataset(imagePaths = imagePaths,\n",
    "                             maskPaths = maskPaths\n",
    "                            )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = 16,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "val_dataset = ValDataset(imagePaths = valImagePaths,\n",
    "                         maskPaths = valMaskPaths,\n",
    "                        )\n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n",
    "                                         batch_size = 16,\n",
    "                                         shuffle = False\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55375b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True), #inplace=True can slightly reduce memory usage\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ConvBlock(x)\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.ConvBlock = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.ConvBlock(x) #skip connection\n",
    "        p = self.pool(s) #pass maxpool2d to next layer of network\n",
    "        return (p, s)\n",
    "    \n",
    "#decoder just for autoencoder, change for UNet\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upConv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)        \n",
    "        self.ConvBlock = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, p):\n",
    "        x = self.upConv(p)\n",
    "        return self.ConvBlock(x)\n",
    "    \n",
    "#decoder for UNet\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.upConv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)        \n",
    "        self.ConvBlock = ConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, p, s):\n",
    "        x = self.upConv(p)\n",
    "        x = torch.cat([x, s], dim=1)\n",
    "        return self.ConvBlock(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179b008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet34\n",
    "        trainedresnet34 = torch.load('/project/trlab/Pretrain Resnet/pretrainedResnetStateDict.pth')\n",
    "        \n",
    "        # Load pre-trained ResNet34 as encoder\n",
    "        self.encoder = models.resnet34()\n",
    "        self.encoder.fc = nn.Linear(512, 4)\n",
    "        self.encoder.load_state_dict(trainedresnet34)\n",
    "        self.encoder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        \n",
    "        # Contracting path (left side of U-Net)\n",
    "        self.conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Expanding path (right side of U-Net)\n",
    "        self.upconv1 = UpBlock(1024, 512)\n",
    "        self.upconv2 = UpBlock(512, 256)\n",
    "        self.upconv3 = UpBlock(256, 128)\n",
    "        self.upconv4 = UpBlock(128, 64)\n",
    "        \n",
    "        self.upConvAgain1 = nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)\n",
    "        self.upConvAgain2 = nn.ConvTranspose2d(256, 256, kernel_size=2, stride=2)\n",
    "        self.upConvAgain3 = nn.ConvTranspose2d(128, 128, kernel_size=2, stride=2)\n",
    "        self.upConvAgain4 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.upConvAgain5 = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2)\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(512,1024)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Contracting path\n",
    "        x = self.encoder.conv1(x)\n",
    "        x = self.encoder.bn1(x)\n",
    "        x = self.encoder.relu(x)\n",
    "        x = self.encoder.maxpool(x)\n",
    "\n",
    "        s1 = self.encoder.layer1(x)\n",
    "        p1 = self.pool(s1)\n",
    "        s2 = self.encoder.layer2(p1)\n",
    "        p2 = self.pool(s2)\n",
    "        s3 = self.encoder.layer3(p2)\n",
    "        p3 = self.pool(s3)\n",
    "        s4 = self.encoder.layer4(p3)\n",
    "        p4 = self.pool(s4)\n",
    "        \n",
    "        b = self.bottleneck(p4)\n",
    "        \n",
    "        # Expanding path with skip connections\n",
    "\n",
    "        \n",
    "        d1 = self.upconv1(b, s4)\n",
    "        d1 = self.upConvAgain1(d1)\n",
    "        d2 = self.upconv2(d1, s3)\n",
    "        d2 = self.upConvAgain2(d2)\n",
    "        d3 = self.upconv3(d2, s2)\n",
    "        d3 = self.upConvAgain3(d3)\n",
    "        d4 = self.upconv4(d3, s1)\n",
    "        d4 = self.upConvAgain4(d4)\n",
    "        \n",
    "        last = self.output_layer(d4)\n",
    "        last = self.upConvAgain5(last)\n",
    "        \n",
    "        return self.activation(last)\n",
    "\n",
    "# Creating an instance of the UNet model\n",
    "model = UNet(num_classes=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c945efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model.to(device)\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "#loss_function = nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 3e-4,\n",
    "                             weight_decay = 1e-8\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a70b5a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 finished\n",
      "epoch 2 finished\n",
      "epoch 3 finished\n",
      "epoch 4 finished\n",
      "epoch 5 finished\n",
      "epoch 6 finished\n",
      "epoch 7 finished\n",
      "epoch 8 finished\n",
      "epoch 9 finished\n",
      "epoch 10 finished\n",
      "epoch 11 finished\n",
      "epoch 12 finished\n",
      "epoch 13 finished\n",
      "epoch 14 finished\n",
      "epoch 15 finished\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.77 GiB total capacity; 14.33 GiB already allocated; 9.12 MiB free; 14.35 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-01814e55c1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mreconstructed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mrunningValLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunningValLoss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cb1c49acac38>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0md3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupConvAgain3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0md4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0md4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupConvAgain4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    914\u001b[0m             input, output_size, self.stride, self.padding, self.kernel_size, self.dilation)  # type: ignore[arg-type]\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m    917\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             output_padding, self.groups, self.dilation)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.77 GiB total capacity; 14.33 GiB already allocated; 9.12 MiB free; 14.35 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "count = 1\n",
    "outputs = []\n",
    "losses = []\n",
    "valLosses=[]\n",
    "#test_losses = []\n",
    "\n",
    "# modify training loop to use automatic mixed precision\n",
    "\n",
    "# convert model to half precision to save memory\n",
    "#model.half()\n",
    "\n",
    "# need to keep batch norm layers at fp32\n",
    "#for layer in model.modules():\n",
    "#    if isinstance(layer, nn.BatchNorm2d):\n",
    "#        layer.float()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    \n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    runningLoss = 0\n",
    "    runningValLoss = 0\n",
    "\n",
    "    for (i,(image, mask)) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Output of UNet\n",
    "            reconstructed = model(image)\n",
    "\n",
    "            # Calculating the loss function\n",
    "            #loss = loss_function(reconstructed, mask)\n",
    "            #loss = dice_loss(reconstructed, mask)\n",
    "            loss = dice_loss(reconstructed, mask)\n",
    "            runningLoss = runningLoss + loss.item()\n",
    "            \n",
    "        # The gradients are set to zero,\n",
    "        # the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    for (j,(image, mask)) in enumerate(val_loader):\n",
    "        model.eval()\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        reconstructed = model(image)\n",
    "        loss = dice_loss(reconstructed, mask)\n",
    "        runningValLoss = runningValLoss + loss.item()\n",
    "        \n",
    "       \n",
    "    # Storing the losses in a list for plotting\n",
    "    losses.append(runningLoss/len(train_loader))\n",
    "    valLosses.append(runningValLoss/len(val_loader))\n",
    "    outputs.append((epochs, image, reconstructed))\n",
    "    print(\"epoch\", count, \"finished\")\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "499ecfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x145cfa7ded60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(15),losses)\n",
    "plt.plot(np.arange(15),valLosses)\n",
    "plt.legend(['Training', 'Validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('scriptedResnetUNet.pt')\n",
    "\n",
    "#after this cell, restart kernel to clear gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0192db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load with:\n",
    "device = 'cuda'\n",
    "model = torch.jit.load('scriptedResnetUnet.pt')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d21a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss functions from\n",
    "# https://towardsdatascience.com/how-accurate-is-image-segmentation-dd448f896388#:~:text=Dice%20Coefficient&text=Dice%20coefficient%20is%20a%20measure,while%200%20indicates%20no%20overlap.&text=Dice%20Loss%20%3D%201%20%E2%80%94%20Dice%20Coefficient.\n",
    "def dice_metric(inputs, target):\n",
    "    intersection = 2.0 * (target * inputs).sum()\n",
    "    union = target.sum() + inputs.sum()\n",
    "    if target.sum() == 0 and inputs.sum() == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "def dice_loss(inputs, target):\n",
    "    num = target.size(0)\n",
    "    inputs = inputs.reshape(num, -1)\n",
    "    target = target.reshape(num, -1)\n",
    "    smooth = 1.0\n",
    "    intersection = (inputs * target)\n",
    "    dice = (2. * intersection.sum(1) + smooth) / (inputs.sum(1) + target.sum(1) + smooth)\n",
    "    dice = 1 - dice.sum() / num\n",
    "    return dice\n",
    "\n",
    "def bce_dice_loss(inputs, target):\n",
    "    # add sigmoid since we take it out to use BCEWithLogitsLoss\n",
    "    dicescore = dice_loss(nn.functional.sigmoid(inputs), target)\n",
    "    bcescore = nn.BCEWithLogitsLoss()\n",
    "    bceloss = bcescore(inputs, target)\n",
    "\n",
    "    return bceloss + dicescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 3e-4,\n",
    "                             weight_decay = 1e-8\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathway for image folder\n",
    "imagePath = \"/project/trlab/imagePath\"\n",
    "maskPath = \"/project/trlab/maskPath\"\n",
    "\n",
    "# try with single image\n",
    "#imagePath = \"/project/trlab/SingleIm\"\n",
    "#maskPath = \"/project/trlab/SingleMask\"\n",
    "\n",
    "imagePaths = []\n",
    "maskPaths = []\n",
    "for data_path in sorted(glob.glob(imagePath + '/*')):\n",
    "    imagePaths.append(data_path)\n",
    "    \n",
    "for data_path in sorted(glob.glob(maskPath + '/*')):\n",
    "    maskPaths.append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, imagePaths, maskPaths):\n",
    "        # init method takes list of image paths, ground truth masks, and transformations as input\n",
    "        self.imagePaths = imagePaths\n",
    "        self.maskPaths = maskPaths\n",
    "        \n",
    "    def transform(self, image, mask):\n",
    "        # standardize to values between 0 and 1 for faster convergence\n",
    "        image = image/255.0\n",
    "        image = image.astype('float16')\n",
    "        \n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        # Transfer to device\n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        # Random crop\n",
    "        i, j, h, w = T.RandomCrop.get_params(image, output_size=(512, 512))\n",
    "        image = TF.crop(image, i, j, h, w)\n",
    "        mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "            \n",
    "        # Random rotation\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.randint(-90,90)\n",
    "            image = TF.rotate(image,angle)\n",
    "            mask = TF.rotate(mask,angle)\n",
    "            \n",
    "        # Random Gaussian Blur\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.gaussian_blur(image, 3, 0.15)\n",
    "            \n",
    "        # Random sharpness\n",
    "        sharpness = random.random()\n",
    "        shift = random.uniform(0.2,1.8)\n",
    "        if sharpness > 0.5:\n",
    "            image = TF.adjust_sharpness(image, shift)\n",
    "            \n",
    "        # Random contrast not sure why this isn't working, current documentation says one channel tensors are fine\n",
    "        # but it's throwing an error saying you need 3 channels\n",
    "        #contrast = random.random()\n",
    "        #if contrast > 0.5:\n",
    "        #    image = TF.adjust_contrast(image, 2)\n",
    "        #elif contrast < 0.25:\n",
    "        #    image = TF.adjust_contrast(image, 0.5)\n",
    "        \n",
    "        # Random brightness\n",
    "        brightness = random.random()\n",
    "        shift = random.uniform(0.2,1.8)\n",
    "        if brightness > 0.5:\n",
    "            image = TF.adjust_brightness(image, shift)\n",
    "            \n",
    "        # Random Affine\n",
    "        shearx = random.randint(-45,45)\n",
    "        sheary = random.randint(-45,45)\n",
    "        transx = random.randint(-20,20)\n",
    "        transy = random.randint(-20,20)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.affine(image, translate=(transx,transy), shear=(shearx,sheary), angle=0, scale=1)\n",
    "            mask = TF.affine(mask, translate=(transx,transy), shear=(shearx,sheary), angle=0, scale=1)\n",
    "            \n",
    "        #image = image.type('torch.HalfTensor')\n",
    "        #mask = image.type('torch.HalfTensor')\n",
    "\n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of image paths in dataset\n",
    "        return len(self.imagePaths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # returns sample from dataset\n",
    "        imagePath = self.imagePaths[idx]\n",
    "        maskPath = self.maskPaths[idx]\n",
    "        \n",
    "        image = cv2.imdecode(np.fromfile(imagePath, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.imdecode(np.fromfile(maskPath, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        # use RandomChoice with only ony composed transform so it applies the same way to both\n",
    "        #transform = T.RandomChoice([T.Compose([T.AutoAugment(T.AutoAugmentPolicy.IMAGENET), T.ToTensor()])])\n",
    "        image, mask = self.transform(image, mask)\n",
    "            \n",
    "        return (image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision import datasets\n",
    "torch.manual_seed(1)\n",
    "\n",
    "dataset = Dataset(imagePaths = imagePaths,\n",
    "                  maskPaths = maskPaths\n",
    "                 )\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                     batch_size = 8,\n",
    "                                     shuffle = True)\n",
    "test_dataset.indices\n",
    "train_dataset.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "count = 1\n",
    "outputs = []\n",
    "losses = []\n",
    "\n",
    "# modify training loop to use automatic mixed precision\n",
    "\n",
    "# convert model to half precision to save memory\n",
    "#model.half()\n",
    "\n",
    "# need to keep batch norm layers at fp32\n",
    "#for layer in model.modules():\n",
    "#    if isinstance(layer, nn.BatchNorm2d):\n",
    "#        layer.float()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    \n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    runningLoss = 0\n",
    "    for (i,(image, mask)) in enumerate(loader):\n",
    "        \n",
    "        image = image.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Output of UNet\n",
    "            reconstructed = model(image)\n",
    "       \n",
    "            # Calculating the loss function\n",
    "            #loss = loss_function(reconstructed, mask)\n",
    "            #loss = dice_loss(reconstructed, mask)\n",
    "            loss = bce_dice_loss(reconstructed, mask)\n",
    "            runningLoss = runningLoss + loss.item()\n",
    "            \n",
    "        # The gradients are set to zero,\n",
    "        # the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "        # autocast backprop\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "    # Storing the losses in a list for plotting\n",
    "    losses.append(runningLoss/len(loader))\n",
    "    outputs.append((epochs, image, reconstructed))\n",
    "    print(\"epoch\", count, \"finished\")\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e828d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
