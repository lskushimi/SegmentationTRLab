{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd14e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# pathway for image folder\n",
    "imagePath = \"/project/trlab/AEImages\"\n",
    "#maskPath = \"C:/Users/lskus/OneDrive/Documents/TR Scripts on the go/maskPath\"\n",
    "\n",
    "imagePaths = []\n",
    "#maskPaths = []\n",
    "for data_path in glob.glob(imagePath + '/*'):\n",
    "    imagePaths.append(data_path)\n",
    "    \n",
    "#for data_path in glob.glob(maskPath + '/*'):\n",
    "#    maskPaths.append(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    #def __init__(self, imagePaths, maskPaths, transforms, maskTransform):\n",
    "    def __init__(self, imagePaths):\n",
    "        # init method takes list of image paths, ground truth masks, and transformations as input\n",
    "        self.imagePaths = imagePaths\n",
    "        #self.transforms = transforms\n",
    "        \n",
    "    def transform(self, image):\n",
    "        # standardize to values between 0 and 1 for faster convergence\n",
    "        image = image/255.0\n",
    "        image = image.astype('float32')\n",
    "        \n",
    "        # Transform to tensor\n",
    "        image = TF.to_tensor(image)\n",
    "        \n",
    "        # Transfer to device\n",
    "        image = image.to(device)\n",
    "        #mask = mask.to(device)\n",
    "        \n",
    "        # Random crop\n",
    "        i, j, h, w = T.RandomCrop.get_params(image, output_size=(512, 512))\n",
    "        image = TF.crop(image, i, j, h, w)\n",
    "        #mask = TF.crop(mask, i, j, h, w)\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            #mask = TF.hflip(mask)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            #mask = TF.vflip(mask)\n",
    "            \n",
    "        # Random rotation\n",
    "        if random.random() > 0.5:\n",
    "            angle = random.randint(-90,90)\n",
    "            image = TF.rotate(image,angle)\n",
    "            #mask = TF.rotate(mask,angle)\n",
    "            \n",
    "        # Random Gaussian Blur (can't do on half precision on cpu)\n",
    "        #if random.random() > 0.5:\n",
    "        #    image = TF.gaussian_blur(image, 3, 0.1)\n",
    "        #    mask = TF.gaussian_blur(mask, 3, 0.1)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        # total number of image paths in dataset\n",
    "        return len(self.imagePaths)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        # returns sample from dataset\n",
    "        imagePath = self.imagePaths[idx]\n",
    "        \n",
    "        image = cv2.imdecode(np.fromfile(imagePath, dtype=np.uint8), cv2.IMREAD_GRAYSCALE)\n",
    "        #image = image.astype('float16')\n",
    "        #image = Image.fromarray(np.uint8(image))\n",
    "        #image = TF.to_tensor(image)\n",
    "        \n",
    "        \n",
    "        image = self.transform(image)\n",
    "            \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f6f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.ConvBlock = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True), #inplace=True can slightly reduce memory usage\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ConvBlock(x)\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.ConvBlock = ConvBlock(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = self.ConvBlock(x) #skip connection\n",
    "        p = self.pool(s) #pass maxpool2d to next layer of network\n",
    "        return (p, s)\n",
    "    \n",
    "#decoder just for autoencoder, change for UNet\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upConv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)        \n",
    "        self.ConvBlock = ConvBlock(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, p):\n",
    "        x = self.upConv(p)\n",
    "        return self.ConvBlock(x)\n",
    "    \n",
    "#decoder for UNet\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.upConv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)        \n",
    "        self.ConvBlock = ConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, p, s):\n",
    "        x = self.upConv(p)\n",
    "        x = torch.cat([x, s], dim=1)\n",
    "        return self.ConvBlock(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47550479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with max pooling layers to match architecture of the UNet\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.encoder1 = EncoderBlock(1, 64)\n",
    "        self.encoder2 = EncoderBlock(64, 128)\n",
    "        self.encoder3 = EncoderBlock(128, 256)\n",
    "        self.encoder4 = EncoderBlock(256, 512)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder1 = DecoderBlock(1024, 512)\n",
    "        self.decoder2 = DecoderBlock(512, 256)\n",
    "        self.decoder3 = DecoderBlock(256, 128)\n",
    "        self.decoder4 = DecoderBlock(128, 64)\n",
    "        \n",
    "        # Final Layer\n",
    "        self.decoded = nn.Sequential(nn.Conv2d(64, 1, 3, padding=1),\n",
    "                                     nn.Sigmoid()\n",
    "                                    )\n",
    "    def forward(self, x):\n",
    "        p1, s1 = self.encoder1(x)\n",
    "        p2, s2 = self.encoder2(p1)\n",
    "        p3, s3 = self.encoder3(p2)\n",
    "        p4, s4 = self.encoder4(p3)\n",
    "        \n",
    "        b = self.bottleneck(p4)\n",
    "        \n",
    "        d1 = self.decoder1(b)\n",
    "        d2 = self.decoder2(d1)\n",
    "        d3 = self.decoder3(d2)\n",
    "        d4 = self.decoder4(d3)\n",
    "        \n",
    "        decoded = self.decoded(d4)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e4f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not normalizing the dataset\n",
    "from torchvision import datasets\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#transforms = T.Compose([T.ToTensor(),\n",
    "#                        T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)\n",
    "#                       ])\n",
    "#maskTransform = T.ToTensor()\n",
    "\n",
    "dataset = Dataset(imagePaths = imagePaths\n",
    "                  #maskPaths = maskPaths,\n",
    "                  #transforms = transforms,\n",
    "                  #maskTransform = maskTransform\n",
    "                 )\n",
    "\n",
    "#train_size = int(0.9 * len(dataset))\n",
    "#test_size = len(dataset) - train_size\n",
    "#train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset = dataset,\n",
    "                                     batch_size = 8,\n",
    "                                     shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6bcb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataset.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7bcce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AE(\n",
       "  (encoder1): EncoderBlock(\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder2): EncoderBlock(\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder3): EncoderBlock(\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (encoder4): EncoderBlock(\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bottleneck): ConvBlock(\n",
       "    (ConvBlock): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): DecoderBlock(\n",
       "    (upConv): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder2): DecoderBlock(\n",
       "    (upConv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder3): DecoderBlock(\n",
       "    (upConv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder4): DecoderBlock(\n",
       "    (upConv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (ConvBlock): ConvBlock(\n",
       "      (ConvBlock): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoded): Sequential(\n",
       "    (0): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "model = AE()\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 1e-4,\n",
    "                             weight_decay = 1e-8)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79e5e87c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/pytorch/1.9.0/install/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 finished\n",
      "epoch 2 finished\n",
      "epoch 3 finished\n",
      "epoch 4 finished\n",
      "epoch 5 finished\n",
      "epoch 6 finished\n",
      "epoch 7 finished\n",
      "epoch 8 finished\n",
      "epoch 9 finished\n",
      "epoch 10 finished\n",
      "epoch 11 finished\n",
      "epoch 12 finished\n",
      "epoch 13 finished\n",
      "epoch 14 finished\n",
      "epoch 15 finished\n",
      "epoch 16 finished\n",
      "epoch 17 finished\n",
      "epoch 18 finished\n",
      "epoch 19 finished\n",
      "epoch 20 finished\n",
      "epoch 21 finished\n",
      "epoch 22 finished\n",
      "epoch 23 finished\n",
      "epoch 24 finished\n",
      "epoch 25 finished\n",
      "epoch 26 finished\n",
      "epoch 27 finished\n",
      "epoch 28 finished\n",
      "epoch 29 finished\n",
      "epoch 30 finished\n",
      "epoch 31 finished\n",
      "epoch 32 finished\n",
      "epoch 33 finished\n",
      "epoch 34 finished\n",
      "epoch 35 finished\n",
      "epoch 36 finished\n",
      "epoch 37 finished\n",
      "epoch 38 finished\n",
      "epoch 39 finished\n",
      "epoch 40 finished\n",
      "epoch 41 finished\n",
      "epoch 42 finished\n",
      "epoch 43 finished\n",
      "epoch 44 finished\n",
      "epoch 45 finished\n",
      "epoch 46 finished\n",
      "epoch 47 finished\n",
      "epoch 48 finished\n",
      "epoch 49 finished\n",
      "epoch 50 finished\n",
      "epoch 51 finished\n",
      "epoch 52 finished\n",
      "epoch 53 finished\n",
      "epoch 54 finished\n",
      "epoch 55 finished\n",
      "epoch 56 finished\n",
      "epoch 57 finished\n",
      "epoch 58 finished\n",
      "epoch 59 finished\n",
      "epoch 60 finished\n",
      "epoch 61 finished\n",
      "epoch 62 finished\n",
      "epoch 63 finished\n",
      "epoch 64 finished\n",
      "epoch 65 finished\n",
      "epoch 66 finished\n",
      "epoch 67 finished\n",
      "epoch 68 finished\n",
      "epoch 69 finished\n",
      "epoch 70 finished\n",
      "epoch 71 finished\n",
      "epoch 72 finished\n",
      "epoch 73 finished\n",
      "epoch 74 finished\n",
      "epoch 75 finished\n",
      "epoch 76 finished\n",
      "epoch 77 finished\n",
      "epoch 78 finished\n",
      "epoch 79 finished\n",
      "epoch 80 finished\n",
      "epoch 81 finished\n",
      "epoch 82 finished\n",
      "epoch 83 finished\n",
      "epoch 84 finished\n",
      "epoch 85 finished\n",
      "epoch 86 finished\n",
      "epoch 87 finished\n",
      "epoch 88 finished\n",
      "epoch 89 finished\n",
      "epoch 90 finished\n",
      "epoch 91 finished\n",
      "epoch 92 finished\n",
      "epoch 93 finished\n",
      "epoch 94 finished\n",
      "epoch 95 finished\n",
      "epoch 96 finished\n",
      "epoch 97 finished\n",
      "epoch 98 finished\n",
      "epoch 99 finished\n",
      "epoch 100 finished\n",
      "epoch 101 finished\n",
      "epoch 102 finished\n",
      "epoch 103 finished\n",
      "epoch 104 finished\n",
      "epoch 105 finished\n",
      "epoch 106 finished\n",
      "epoch 107 finished\n",
      "epoch 108 finished\n",
      "epoch 109 finished\n",
      "epoch 110 finished\n",
      "epoch 111 finished\n",
      "epoch 112 finished\n",
      "epoch 113 finished\n",
      "epoch 114 finished\n",
      "epoch 115 finished\n",
      "epoch 116 finished\n",
      "epoch 117 finished\n",
      "epoch 118 finished\n",
      "epoch 119 finished\n",
      "epoch 120 finished\n",
      "epoch 121 finished\n",
      "epoch 122 finished\n",
      "epoch 123 finished\n",
      "epoch 124 finished\n",
      "epoch 125 finished\n",
      "epoch 126 finished\n",
      "epoch 127 finished\n",
      "epoch 128 finished\n",
      "epoch 129 finished\n",
      "epoch 130 finished\n",
      "epoch 131 finished\n",
      "epoch 132 finished\n",
      "epoch 133 finished\n",
      "epoch 134 finished\n",
      "epoch 135 finished\n",
      "epoch 136 finished\n",
      "epoch 137 finished\n",
      "epoch 138 finished\n",
      "epoch 139 finished\n",
      "epoch 140 finished\n",
      "epoch 141 finished\n",
      "epoch 142 finished\n",
      "epoch 143 finished\n",
      "epoch 144 finished\n",
      "epoch 145 finished\n",
      "epoch 146 finished\n",
      "epoch 147 finished\n",
      "epoch 148 finished\n",
      "epoch 149 finished\n",
      "epoch 150 finished\n",
      "epoch 151 finished\n",
      "epoch 152 finished\n",
      "epoch 153 finished\n",
      "epoch 154 finished\n",
      "epoch 155 finished\n",
      "epoch 156 finished\n",
      "epoch 157 finished\n",
      "epoch 158 finished\n",
      "epoch 159 finished\n",
      "epoch 160 finished\n",
      "epoch 161 finished\n",
      "epoch 162 finished\n",
      "epoch 163 finished\n",
      "epoch 164 finished\n",
      "epoch 165 finished\n",
      "epoch 166 finished\n",
      "epoch 167 finished\n",
      "epoch 168 finished\n",
      "epoch 169 finished\n",
      "epoch 170 finished\n",
      "epoch 171 finished\n",
      "epoch 172 finished\n",
      "epoch 173 finished\n",
      "epoch 174 finished\n",
      "epoch 175 finished\n",
      "epoch 176 finished\n",
      "epoch 177 finished\n",
      "epoch 178 finished\n",
      "epoch 179 finished\n",
      "epoch 180 finished\n",
      "epoch 181 finished\n",
      "epoch 182 finished\n",
      "epoch 183 finished\n",
      "epoch 184 finished\n",
      "epoch 185 finished\n",
      "epoch 186 finished\n",
      "epoch 187 finished\n",
      "epoch 188 finished\n",
      "epoch 189 finished\n",
      "epoch 190 finished\n",
      "epoch 191 finished\n",
      "epoch 192 finished\n",
      "epoch 193 finished\n",
      "epoch 194 finished\n",
      "epoch 195 finished\n",
      "epoch 196 finished\n",
      "epoch 197 finished\n",
      "epoch 198 finished\n",
      "epoch 199 finished\n",
      "epoch 200 finished\n",
      "epoch 201 finished\n",
      "epoch 202 finished\n",
      "epoch 203 finished\n",
      "epoch 204 finished\n",
      "epoch 205 finished\n",
      "epoch 206 finished\n",
      "epoch 207 finished\n",
      "epoch 208 finished\n",
      "epoch 209 finished\n",
      "epoch 210 finished\n",
      "epoch 211 finished\n",
      "epoch 212 finished\n",
      "epoch 213 finished\n",
      "epoch 214 finished\n",
      "epoch 215 finished\n",
      "epoch 216 finished\n",
      "epoch 217 finished\n",
      "epoch 218 finished\n",
      "epoch 219 finished\n",
      "epoch 220 finished\n",
      "epoch 221 finished\n",
      "epoch 222 finished\n",
      "epoch 223 finished\n",
      "epoch 224 finished\n",
      "epoch 225 finished\n",
      "epoch 226 finished\n",
      "epoch 227 finished\n",
      "epoch 228 finished\n",
      "epoch 229 finished\n",
      "epoch 230 finished\n",
      "epoch 231 finished\n",
      "epoch 232 finished\n",
      "epoch 233 finished\n",
      "epoch 234 finished\n",
      "epoch 235 finished\n",
      "epoch 236 finished\n",
      "epoch 237 finished\n",
      "epoch 238 finished\n",
      "epoch 239 finished\n",
      "epoch 240 finished\n",
      "epoch 241 finished\n",
      "epoch 242 finished\n",
      "epoch 243 finished\n",
      "epoch 244 finished\n",
      "epoch 245 finished\n",
      "epoch 246 finished\n",
      "epoch 247 finished\n",
      "epoch 248 finished\n",
      "epoch 249 finished\n",
      "epoch 250 finished\n",
      "epoch 251 finished\n",
      "epoch 252 finished\n",
      "epoch 253 finished\n",
      "epoch 254 finished\n",
      "epoch 255 finished\n",
      "epoch 256 finished\n",
      "epoch 257 finished\n",
      "epoch 258 finished\n",
      "epoch 259 finished\n",
      "epoch 260 finished\n",
      "epoch 261 finished\n",
      "epoch 262 finished\n",
      "epoch 263 finished\n",
      "epoch 264 finished\n",
      "epoch 265 finished\n",
      "epoch 266 finished\n",
      "epoch 267 finished\n",
      "epoch 268 finished\n",
      "epoch 269 finished\n",
      "epoch 270 finished\n",
      "epoch 271 finished\n",
      "epoch 272 finished\n",
      "epoch 273 finished\n",
      "epoch 274 finished\n",
      "epoch 275 finished\n",
      "epoch 276 finished\n",
      "epoch 277 finished\n",
      "epoch 278 finished\n",
      "epoch 279 finished\n",
      "epoch 280 finished\n",
      "epoch 281 finished\n",
      "epoch 282 finished\n",
      "epoch 283 finished\n",
      "epoch 284 finished\n",
      "epoch 285 finished\n",
      "epoch 286 finished\n",
      "epoch 287 finished\n",
      "epoch 288 finished\n",
      "epoch 289 finished\n",
      "epoch 290 finished\n",
      "epoch 291 finished\n",
      "epoch 292 finished\n",
      "epoch 293 finished\n",
      "epoch 294 finished\n",
      "epoch 295 finished\n",
      "epoch 296 finished\n",
      "epoch 297 finished\n",
      "epoch 298 finished\n",
      "epoch 299 finished\n",
      "epoch 300 finished\n",
      "epoch 301 finished\n",
      "epoch 302 finished\n",
      "epoch 303 finished\n",
      "epoch 304 finished\n",
      "epoch 305 finished\n",
      "epoch 306 finished\n",
      "epoch 307 finished\n",
      "epoch 308 finished\n",
      "epoch 309 finished\n",
      "epoch 310 finished\n",
      "epoch 311 finished\n",
      "epoch 312 finished\n",
      "epoch 313 finished\n",
      "epoch 314 finished\n",
      "epoch 315 finished\n",
      "epoch 316 finished\n",
      "epoch 317 finished\n",
      "epoch 318 finished\n",
      "epoch 319 finished\n",
      "epoch 320 finished\n",
      "epoch 321 finished\n",
      "epoch 322 finished\n",
      "epoch 323 finished\n",
      "epoch 324 finished\n",
      "epoch 325 finished\n",
      "epoch 326 finished\n",
      "epoch 327 finished\n",
      "epoch 328 finished\n",
      "epoch 329 finished\n",
      "epoch 330 finished\n",
      "epoch 331 finished\n",
      "epoch 332 finished\n",
      "epoch 333 finished\n",
      "epoch 334 finished\n",
      "epoch 335 finished\n",
      "epoch 336 finished\n",
      "epoch 337 finished\n",
      "epoch 338 finished\n",
      "epoch 339 finished\n",
      "epoch 340 finished\n",
      "epoch 341 finished\n",
      "epoch 342 finished\n",
      "epoch 343 finished\n",
      "epoch 344 finished\n",
      "epoch 345 finished\n",
      "epoch 346 finished\n",
      "epoch 347 finished\n",
      "epoch 348 finished\n",
      "epoch 349 finished\n",
      "epoch 350 finished\n",
      "epoch 351 finished\n",
      "epoch 352 finished\n",
      "epoch 353 finished\n",
      "epoch 354 finished\n",
      "epoch 355 finished\n",
      "epoch 356 finished\n",
      "epoch 357 finished\n",
      "epoch 358 finished\n",
      "epoch 359 finished\n",
      "epoch 360 finished\n",
      "epoch 361 finished\n",
      "epoch 362 finished\n",
      "epoch 363 finished\n",
      "epoch 364 finished\n",
      "epoch 365 finished\n",
      "epoch 366 finished\n",
      "epoch 367 finished\n",
      "epoch 368 finished\n",
      "epoch 369 finished\n",
      "epoch 370 finished\n",
      "epoch 371 finished\n",
      "epoch 372 finished\n",
      "epoch 373 finished\n",
      "epoch 374 finished\n",
      "epoch 375 finished\n",
      "epoch 376 finished\n",
      "epoch 377 finished\n",
      "epoch 378 finished\n",
      "epoch 379 finished\n",
      "epoch 380 finished\n",
      "epoch 381 finished\n",
      "epoch 382 finished\n",
      "epoch 383 finished\n",
      "epoch 384 finished\n",
      "epoch 385 finished\n",
      "epoch 386 finished\n",
      "epoch 387 finished\n",
      "epoch 388 finished\n",
      "epoch 389 finished\n",
      "epoch 390 finished\n",
      "epoch 391 finished\n",
      "epoch 392 finished\n",
      "epoch 393 finished\n",
      "epoch 394 finished\n",
      "epoch 395 finished\n",
      "epoch 396 finished\n",
      "epoch 397 finished\n",
      "epoch 398 finished\n",
      "epoch 399 finished\n",
      "epoch 400 finished\n",
      "epoch 401 finished\n",
      "epoch 402 finished\n",
      "epoch 403 finished\n",
      "epoch 404 finished\n",
      "epoch 405 finished\n",
      "epoch 406 finished\n",
      "epoch 407 finished\n",
      "epoch 408 finished\n",
      "epoch 409 finished\n",
      "epoch 410 finished\n",
      "epoch 411 finished\n",
      "epoch 412 finished\n",
      "epoch 413 finished\n",
      "epoch 414 finished\n",
      "epoch 415 finished\n",
      "epoch 416 finished\n",
      "epoch 417 finished\n",
      "epoch 418 finished\n",
      "epoch 419 finished\n",
      "epoch 420 finished\n",
      "epoch 421 finished\n",
      "epoch 422 finished\n",
      "epoch 423 finished\n",
      "epoch 424 finished\n",
      "epoch 425 finished\n",
      "epoch 426 finished\n",
      "epoch 427 finished\n",
      "epoch 428 finished\n",
      "epoch 429 finished\n",
      "epoch 430 finished\n",
      "epoch 431 finished\n",
      "epoch 432 finished\n",
      "epoch 433 finished\n",
      "epoch 434 finished\n",
      "epoch 435 finished\n",
      "epoch 436 finished\n",
      "epoch 437 finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 438 finished\n",
      "epoch 439 finished\n",
      "epoch 440 finished\n",
      "epoch 441 finished\n",
      "epoch 442 finished\n",
      "epoch 443 finished\n",
      "epoch 444 finished\n",
      "epoch 445 finished\n",
      "epoch 446 finished\n",
      "epoch 447 finished\n",
      "epoch 448 finished\n",
      "epoch 449 finished\n",
      "epoch 450 finished\n",
      "epoch 451 finished\n",
      "epoch 452 finished\n",
      "epoch 453 finished\n",
      "epoch 454 finished\n",
      "epoch 455 finished\n",
      "epoch 456 finished\n",
      "epoch 457 finished\n",
      "epoch 458 finished\n",
      "epoch 459 finished\n",
      "epoch 460 finished\n",
      "epoch 461 finished\n",
      "epoch 462 finished\n",
      "epoch 463 finished\n",
      "epoch 464 finished\n",
      "epoch 465 finished\n",
      "epoch 466 finished\n",
      "epoch 467 finished\n",
      "epoch 468 finished\n",
      "epoch 469 finished\n",
      "epoch 470 finished\n",
      "epoch 471 finished\n",
      "epoch 472 finished\n",
      "epoch 473 finished\n",
      "epoch 474 finished\n",
      "epoch 475 finished\n",
      "epoch 476 finished\n",
      "epoch 477 finished\n",
      "epoch 478 finished\n",
      "epoch 479 finished\n",
      "epoch 480 finished\n",
      "epoch 481 finished\n",
      "epoch 482 finished\n",
      "epoch 483 finished\n",
      "epoch 484 finished\n",
      "epoch 485 finished\n",
      "epoch 486 finished\n",
      "epoch 487 finished\n",
      "epoch 488 finished\n",
      "epoch 489 finished\n",
      "epoch 490 finished\n",
      "epoch 491 finished\n",
      "epoch 492 finished\n",
      "epoch 493 finished\n",
      "epoch 494 finished\n",
      "epoch 495 finished\n",
      "epoch 496 finished\n",
      "epoch 497 finished\n",
      "epoch 498 finished\n",
      "epoch 499 finished\n",
      "epoch 500 finished\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = 500\n",
    "count = 1\n",
    "outputs = []\n",
    "losses = []\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    runningLoss = 0\n",
    "    for (image) in loader:\n",
    "        \n",
    "        image = image.to(device)\n",
    "        \n",
    "        # Output of Autoencoder\n",
    "        reconstructed = model(image)\n",
    "       \n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, image)\n",
    "        runningLoss = runningLoss + loss.item()\n",
    "       \n",
    "        # The gradients are set to zero,\n",
    "        # the gradient is computed and stored.\n",
    "        # .step() performs parameter update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "    # Storing the losses in a list for plotting\n",
    "    losses.append(runningLoss/len(loader))\n",
    "    outputs.append((epochs, image, reconstructed))\n",
    "    print(\"epoch\", count, \"finished\")\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "300eec19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2afa0e40c520>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(epochs),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1763222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using aicsimageio to import each frame of the CZI file as a Tiff\n",
    "from aicsimageio.readers import CziReader\n",
    "from aicsimageio import AICSImage\n",
    "from aicsimageio.writers import OmeTiffWriter\n",
    "from aicsimageio.readers import OmeTiffReader\n",
    "model.eval()\n",
    "testInput1 = \"C:/Users/lskus/OneDrive/Documents/TR Scripts on the go/imagePath/ATP ajusted f1.tif\"\n",
    "#test1 = cv2.imdecode(np.fromfile(testInput1, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "tensorTransform = T.ToTensor()\n",
    "reader = AICSImage(testInput1)\n",
    "print(reader.shape)\n",
    "test1 = reader.get_image_data(\"YX\")\n",
    "img_tensor = tensorTransform(test1).to('cuda').unsqueeze(0)\n",
    "output = model(img_tensor)\n",
    "output = output.squeeze()\n",
    "#output = output.permute((1,2,0))\n",
    "output = output.detach().cpu()\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12253dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b2398dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"/project/trlab/AE_NormalizedImages_04LR.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dec9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"C:/Users/lskus/OneDrive/Documents/TR Scripts on the go/AE4.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7de851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
